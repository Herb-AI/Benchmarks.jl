{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSB2 Benchmark set\n",
    "\n",
    "The full benchmark set can be downloaded from [this website](https://zenodo.org/records/5084812). For each problem task, there are two sets of input-output examples: `edge` is a set of some edge cases (roughly 10-40 instances) and `random` is set of a million instances. Due to space limitations, our `data.jl` problems only contain the edge sets for each problem. With this notebook you can convert all the instances from the `random` sets also to benchmark problems to use in Herb.jl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the full benchmark set from the website, using their Python wrapper. To do this, we need the Python package `psb2` which only exists in Pip and not in Conda itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Conda, PyCall\n",
    "using HerbData, HerbBenchmarks\n",
    "\n",
    "Conda.pip_interop(true)\n",
    "Conda.pip(\"install\", \"psb2\")\n",
    "\n",
    "psb2 = PyCall.pyimport(\"psb2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a function for writing creating data files per problem in the benchmark. This method downloads the JSON files into the `datasets` subfolder, which is ignored. For each problem, the `random` set contains one million examples.\n",
    "\n",
    "For one problem, getting the full `random` set takes about five minutes. In total, there are 25 problems in the benchmark. Getting the `edge` sets for all benchmark problems takes about 5 minutes in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function write_psb2_problems_to_file(problems::Vector{String}=String[\"fizz-buzz\"], edge_or_random=\"random\", n_train=200, n_test=2000, format=\"psb2\")\n",
    "    # If no specific problem specified, get all problems in the benchmark\n",
    "    if isempty(problems) \n",
    "        problems = psb2.PROBLEMS\n",
    "    end\n",
    "    for name in problems\n",
    "        if !(name in psb2.PROBLEMS)\n",
    "            throw(ArgumentError(\"$(name) does not exist in the psb2 problems\"))\n",
    "        else\n",
    "            # This loads the json files to the /datasets/<name> folder\n",
    "            psb2.fetch_examples(pwd(), name, n_train, n_test, format)\n",
    "            julia_name = replace(name, \"-\" => \"_\")\n",
    "            # Reset the file if it exsits, so we can append the data all at once\n",
    "            if isfile(\"$(pwd())/datasets/$(name)/$(julia_name)data.jl\")\n",
    "                rm(\"$(pwd())/datasets/$(name)/$(julia_name)data.jl\")\n",
    "            end\n",
    "            parse_to_julia(\"$(pwd())/datasets/$(name)/\", \"$(name)-$(edge_or_random).json\", PSB2_2021.parse_line_json, julia_name, \"a\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_psb2_problems_to_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
